{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**写在前面:**, 这次新闻文本分类的比赛是由DataWhale与阿里天池联合举办，定位为nlp入门级赛事，[这里有详细的赛题说明](https://tianchi.aliyun.com/competition/entrance/531810/information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题置顶\n",
    "- 怎么选择df某一行的内容并将其转换为string类型\n",
    "- 须要分batch训练吗，分batch的目的是什么？\n",
    "- sklearn中分类器，比如LR对train_X 和 train_y 有什么要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 赛题理解\n",
    "- 任务目标：文本分类（14类）\n",
    "- 数据格式：每条样本由text和label组成；text进行了字符级的匿名处理（由数字表示原来的字符），label由0~13这14个数字组成，表示财经等14类文本。\n",
    "- 评价标准：f1  越高越好\n",
    "- 可选思路：TF-IDF+LR等传统分类模型；word2vec做特征+RNN+softmax；bert做特征+softmax分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task2 数据读取与数据分析——on 0722\n",
    "- 查看训练集结构及规模\n",
    "- 获取最大text长度\n",
    "- 建立词典，获取词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label\\ttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2\\t2967 6758 339 2021 1854 3731 4109 3792 4149...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11\\t4464 486 6352 5619 2465 4802 1452 3137 577...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3\\t7346 4068 5074 3747 5681 6093 1777 2226 735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2\\t7159 948 4866 2109 5520 2490 211 3956 5520 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3\\t3646 3055 3055 2490 4659 6065 3370 5814 246...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         label\\ttext\n",
       "0  2\\t2967 6758 339 2021 1854 3731 4109 3792 4149...\n",
       "1  11\\t4464 486 6352 5619 2465 4802 1452 3137 577...\n",
       "2  3\\t7346 4068 5074 3747 5681 6093 1777 2226 735...\n",
       "3  2\\t7159 948 4866 2109 5520 2490 211 3956 5520 ...\n",
       "4  3\\t3646 3055 3055 2490 4659 6065 3370 5814 246..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'train_set.csv/train_set.csv'\n",
    "train_set = pd.read_csv(train_path, encoding = 'utf-8')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 1 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   label\ttext  200000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#看一下多少样本，每个样本几个属性\n",
    "train_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分别得到train_text 和 train_label\n",
    "\n",
    "#train_set.loc[0]['label\\ttext']\n",
    "train_text, train_label = [],[]\n",
    "for i in range(20000):\n",
    "    tmp = train_set.loc[i]['label\\ttext'].split('\\t')\n",
    "    a, b = tmp[1], tmp[0]\n",
    "    train_label.append(b)\n",
    "    train_text.append(a)\n",
    "    \n",
    "#将test list化\n",
    "train_text_lst = []\n",
    "for i in range(20000):\n",
    "    train_text_lst.append(train_text[i].split())\n",
    "len(train_text_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44665"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取最大的text长度\n",
    "def get_len_text(text_list):\n",
    "    Max = 0\n",
    "    for i, it in enumerate(text_list):\n",
    "        Max= len(it) if Max<len(it) else Max\n",
    "    return Max\n",
    "get_len_text(train_text_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有18092357个words.\n"
     ]
    }
   ],
   "source": [
    "#先把所有word都放到all_words里面\n",
    "all_words = []\n",
    "for i in train_text_lst:\n",
    "    for j in i:\n",
    "        all_words.append(j)\n",
    "print(\"一共有\"+str(len(all_words))+\"个words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词典中包含 5697 个单词\n",
      "339\n",
      "2231\n"
     ]
    }
   ],
   "source": [
    "#建立词典 词典中的每一个item()是 word:[id, freq]\n",
    "from collections import Counter\n",
    "def build_vocab(all_words):\n",
    "    cnt = Counter(all_words) #得到一个字典 word:freq\n",
    "    res = {}\n",
    "    for word,freq in cnt.items():\n",
    "        res[word] = [len(res), freq]\n",
    "    return res\n",
    "print(\"词典中包含 \" + str(len(build_vocab(all_words))) + \" 个单词\" )\n",
    "\n",
    "#词典 voc\n",
    "voc = build_vocab(all_words)\n",
    "                  \n",
    "def id2word(id,voc):\n",
    "    for word,lst in voc.items():\n",
    "        if lst[0] == id:\n",
    "            return word\n",
    "def word2id(word,voc):\n",
    "    return voc[word][0]\n",
    "print(id2word(2,voc))\n",
    "print(word2id('667',voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id('339',voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task3  基于机器学习的文本分类 ——on 0725\n",
    "- 构建特征（采用word_count / TF-IDF两种方式）  \n",
    "- 尝试用sklearn库中的lr svm xgb 进行分类 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 encoding by wordcnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_by_wordcnt(train_set,voc):\n",
    "    res = []\n",
    "    for it in train_set:\n",
    "        tmp = [0]*len(voc)\n",
    "        for i in it:\n",
    "            index = word2id(i,voc)\n",
    "            tmp[index]+=1\n",
    "        res.append(tmp)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存encoding 后的train_set\n",
    "train_encoding_by_wordcnt = encoding_by_wordcnt(train_text_lst,voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_encoding_by_wordcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_set_encooding_by_wordcnt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_encoding(encoding_lst,csv_path):\n",
    "    df = pd.DataFrame(encoding_lst)\n",
    "    df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载并且放到list里\n",
    "def load_encoding(csv_path):\n",
    "    df =  pd.read_csv(csv_path)\n",
    "    res = []\n",
    "    for i in range(df.shape[0]):\n",
    "        lst = df.loc[0].to_list()\n",
    "        res.append(lst)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_model 并 训练\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def build_and_train_encoding_by_wordcnt(train_x,train_y):\n",
    "    lr = LogisticRegression()\n",
    "#train_x,train_y = train_encoding_by_wordcnt,train_label\n",
    "    lr.fit(train_x, train_y)\n",
    "    return lr   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估函数\n",
    "import sklearn.metrics\n",
    "def model_metrics(model, test_x, test_y):\n",
    "    y_pred = model.predict(test_x)\n",
    "    return sklearn.metrics.f1_score (test_y,y_pred,labels = [i for i in range(14)], average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集与测试集，\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x,train_y = train_encoding_by_wordcnt,train_label\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x,train_y,test_size=0.3, random_state = 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AppData\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = build_and_train_encoding_by_wordcnt(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AppData\\Miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:565: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8445580663131584"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metrics(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 encoding by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "#查看数据结构 tfidf[i][j]表示i类文本中的tf-idf权重  \n",
    "def encoding_by_tfidf(X):\n",
    "    transformer = TfidfTransformer() \n",
    "#将词频矩阵X统计成TF-IDF值  \n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoding_by_tfidf = encoding_by_tfidf(train_encoding_by_wordcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tfidf,train_y_tfidf = train_encoding_by_tfidf.toarray(),train_label\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(train_x_tfidf,train_y_tfidf,test_size=0.3, random_state = 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AppData\\Miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf = build_and_train_encoding_by_wordcnt(X_train_tfidf,y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AppData\\Miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:565: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8622178600615233"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_metrics(lr_tfidf, X_test_tfidf, y_test_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
